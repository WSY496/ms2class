{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshitaka-i/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### reproducibility for Keras\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(7)\n",
    "rn.seed(7)\n",
    "\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1,\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=\"1\", \n",
    "        allow_growth=True,\n",
    "#         per_process_gpu_memory_fraction=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(7)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "##### Import some librarys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, GaussianNoise, GaussianDropout\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from  keras.regularizers import l1, l2\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('../data/feature_selection_positive.csv')\n",
    "\n",
    "\n",
    "# divide objective and target\n",
    "objective = df.Subclass\n",
    "le = preprocessing.LabelEncoder()\n",
    "objective = le.fit_transform(objective)\n",
    "features = df.drop('Subclass', axis=1)\n",
    "\n",
    "# train test split\n",
    "random_state=np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    objective,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# transform  for keras's target label\n",
    "y_train_for_keras = np_utils.to_categorical(y_train)\n",
    "y_test_for_keras = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1340 samples, validate on 336 samples\n",
      "Epoch 1/100\n",
      "1340/1340 [==============================] - 2s 1ms/step - loss: 1.8700 - acc: 0.2507 - val_loss: 1.4152 - val_acc: 0.3155\n",
      "Epoch 2/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 1.7653 - acc: 0.2582 - val_loss: 1.3522 - val_acc: 0.2976\n",
      "Epoch 3/100\n",
      "1340/1340 [==============================] - 0s 67us/step - loss: 1.6996 - acc: 0.2813 - val_loss: 1.3382 - val_acc: 0.2887\n",
      "Epoch 4/100\n",
      "1340/1340 [==============================] - 0s 69us/step - loss: 1.6141 - acc: 0.2985 - val_loss: 1.3362 - val_acc: 0.2976\n",
      "Epoch 5/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 1.5480 - acc: 0.3075 - val_loss: 1.3222 - val_acc: 0.3065\n",
      "Epoch 6/100\n",
      "1340/1340 [==============================] - 0s 69us/step - loss: 1.5700 - acc: 0.3299 - val_loss: 1.3119 - val_acc: 0.3214\n",
      "Epoch 7/100\n",
      "1340/1340 [==============================] - 0s 67us/step - loss: 1.5446 - acc: 0.3209 - val_loss: 1.2941 - val_acc: 0.4286\n",
      "Epoch 8/100\n",
      "1340/1340 [==============================] - 0s 69us/step - loss: 1.5496 - acc: 0.3328 - val_loss: 1.2672 - val_acc: 0.4256\n",
      "Epoch 9/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 1.5117 - acc: 0.3418 - val_loss: 1.2500 - val_acc: 0.4196\n",
      "Epoch 10/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 1.4626 - acc: 0.3448 - val_loss: 1.2299 - val_acc: 0.4405\n",
      "Epoch 11/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 1.4028 - acc: 0.3627 - val_loss: 1.2163 - val_acc: 0.4435\n",
      "Epoch 12/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 1.3644 - acc: 0.3836 - val_loss: 1.2061 - val_acc: 0.5536\n",
      "Epoch 13/100\n",
      "1340/1340 [==============================] - 0s 67us/step - loss: 1.3534 - acc: 0.3940 - val_loss: 1.2001 - val_acc: 0.4673\n",
      "Epoch 14/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 1.3120 - acc: 0.3918 - val_loss: 1.1944 - val_acc: 0.4524\n",
      "Epoch 15/100\n",
      "1340/1340 [==============================] - 0s 86us/step - loss: 1.3228 - acc: 0.4030 - val_loss: 1.1901 - val_acc: 0.4405\n",
      "Epoch 16/100\n",
      "1340/1340 [==============================] - 0s 84us/step - loss: 1.2804 - acc: 0.4127 - val_loss: 1.1826 - val_acc: 0.4583\n",
      "Epoch 17/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 1.3250 - acc: 0.3843 - val_loss: 1.1758 - val_acc: 0.4643\n",
      "Epoch 18/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 1.2540 - acc: 0.4410 - val_loss: 1.1649 - val_acc: 0.5238\n",
      "Epoch 19/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 1.2344 - acc: 0.4328 - val_loss: 1.1559 - val_acc: 0.5030\n",
      "Epoch 20/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 1.2179 - acc: 0.4560 - val_loss: 1.1468 - val_acc: 0.4702\n",
      "Epoch 21/100\n",
      "1340/1340 [==============================] - 0s 68us/step - loss: 1.2227 - acc: 0.4724 - val_loss: 1.1417 - val_acc: 0.4524\n",
      "Epoch 22/100\n",
      "1340/1340 [==============================] - 0s 69us/step - loss: 1.1984 - acc: 0.4612 - val_loss: 1.1346 - val_acc: 0.4554\n",
      "Epoch 23/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 1.2089 - acc: 0.4582 - val_loss: 1.1282 - val_acc: 0.4613\n",
      "Epoch 24/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 1.1637 - acc: 0.5000 - val_loss: 1.1188 - val_acc: 0.4702\n",
      "Epoch 25/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 1.1362 - acc: 0.4881 - val_loss: 1.1061 - val_acc: 0.5030\n",
      "Epoch 26/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 1.1240 - acc: 0.4963 - val_loss: 1.0932 - val_acc: 0.5357\n",
      "Epoch 27/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 1.1194 - acc: 0.5127 - val_loss: 1.0823 - val_acc: 0.5387\n",
      "Epoch 28/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 1.0968 - acc: 0.5060 - val_loss: 1.0723 - val_acc: 0.5327\n",
      "Epoch 29/100\n",
      "1340/1340 [==============================] - 0s 73us/step - loss: 1.0987 - acc: 0.5157 - val_loss: 1.0630 - val_acc: 0.5476\n",
      "Epoch 30/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 1.0735 - acc: 0.5187 - val_loss: 1.0543 - val_acc: 0.5565\n",
      "Epoch 31/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 1.0488 - acc: 0.5179 - val_loss: 1.0462 - val_acc: 0.5595\n",
      "Epoch 32/100\n",
      "1340/1340 [==============================] - 0s 91us/step - loss: 1.0573 - acc: 0.5254 - val_loss: 1.0370 - val_acc: 0.5685\n",
      "Epoch 33/100\n",
      "1340/1340 [==============================] - 0s 81us/step - loss: 1.0293 - acc: 0.5448 - val_loss: 1.0281 - val_acc: 0.5744\n",
      "Epoch 34/100\n",
      "1340/1340 [==============================] - 0s 73us/step - loss: 1.0480 - acc: 0.5231 - val_loss: 1.0214 - val_acc: 0.5685\n",
      "Epoch 35/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 0.9915 - acc: 0.5470 - val_loss: 1.0190 - val_acc: 0.5446\n",
      "Epoch 36/100\n",
      "1340/1340 [==============================] - 0s 98us/step - loss: 0.9933 - acc: 0.5537 - val_loss: 1.0104 - val_acc: 0.5476\n",
      "Epoch 37/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 0.9695 - acc: 0.5590 - val_loss: 1.0005 - val_acc: 0.5595\n",
      "Epoch 38/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 0.9331 - acc: 0.6067 - val_loss: 0.9907 - val_acc: 0.5506\n",
      "Epoch 39/100\n",
      "1340/1340 [==============================] - 0s 82us/step - loss: 0.9080 - acc: 0.5896 - val_loss: 0.9780 - val_acc: 0.5625\n",
      "Epoch 40/100\n",
      "1340/1340 [==============================] - 0s 68us/step - loss: 0.9940 - acc: 0.5799 - val_loss: 0.9660 - val_acc: 0.5923\n",
      "Epoch 41/100\n",
      "1340/1340 [==============================] - 0s 78us/step - loss: 0.9224 - acc: 0.5843 - val_loss: 0.9567 - val_acc: 0.6310\n",
      "Epoch 42/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 0.9199 - acc: 0.5813 - val_loss: 0.9488 - val_acc: 0.6250\n",
      "Epoch 43/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.9058 - acc: 0.6022 - val_loss: 0.9421 - val_acc: 0.6131\n",
      "Epoch 44/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 0.8603 - acc: 0.6142 - val_loss: 0.9376 - val_acc: 0.5863\n",
      "Epoch 45/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 0.8692 - acc: 0.6164 - val_loss: 0.9378 - val_acc: 0.5804\n",
      "Epoch 46/100\n",
      "1340/1340 [==============================] - 0s 80us/step - loss: 0.8518 - acc: 0.6157 - val_loss: 0.9410 - val_acc: 0.5714\n",
      "Epoch 47/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 0.8447 - acc: 0.6433 - val_loss: 0.9427 - val_acc: 0.5655\n",
      "Epoch 48/100\n",
      "1340/1340 [==============================] - 0s 67us/step - loss: 0.8419 - acc: 0.6351 - val_loss: 0.9423 - val_acc: 0.5595\n",
      "Epoch 49/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.8071 - acc: 0.6537 - val_loss: 0.9356 - val_acc: 0.5952\n",
      "Epoch 50/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.8473 - acc: 0.6396 - val_loss: 0.9232 - val_acc: 0.6042\n",
      "Epoch 51/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 0.7814 - acc: 0.6642 - val_loss: 0.9113 - val_acc: 0.5952\n",
      "Epoch 52/100\n",
      "1340/1340 [==============================] - 0s 84us/step - loss: 0.7731 - acc: 0.6604 - val_loss: 0.9015 - val_acc: 0.5923\n",
      "Epoch 53/100\n",
      "1340/1340 [==============================] - 0s 81us/step - loss: 0.7423 - acc: 0.6701 - val_loss: 0.8959 - val_acc: 0.5952\n",
      "Epoch 54/100\n",
      "1340/1340 [==============================] - 0s 85us/step - loss: 0.7453 - acc: 0.6701 - val_loss: 0.8909 - val_acc: 0.5893\n",
      "Epoch 55/100\n",
      "1340/1340 [==============================] - 0s 79us/step - loss: 0.7452 - acc: 0.6918 - val_loss: 0.8834 - val_acc: 0.5923\n",
      "Epoch 56/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 0.7367 - acc: 0.6903 - val_loss: 0.8738 - val_acc: 0.6369\n",
      "Epoch 57/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 0.7312 - acc: 0.6955 - val_loss: 0.8676 - val_acc: 0.6548\n",
      "Epoch 58/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 0.6539 - acc: 0.7231 - val_loss: 0.8652 - val_acc: 0.6696\n",
      "Epoch 59/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.6646 - acc: 0.7216 - val_loss: 0.8641 - val_acc: 0.6845\n",
      "Epoch 60/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 0.6562 - acc: 0.7090 - val_loss: 0.8641 - val_acc: 0.6845\n",
      "Epoch 61/100\n",
      "1340/1340 [==============================] - 0s 89us/step - loss: 0.6338 - acc: 0.7396 - val_loss: 0.8670 - val_acc: 0.6786\n",
      "Epoch 62/100\n",
      "1340/1340 [==============================] - 0s 73us/step - loss: 0.6181 - acc: 0.7299 - val_loss: 0.8718 - val_acc: 0.6518\n",
      "Epoch 63/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 0.5973 - acc: 0.7418 - val_loss: 0.8769 - val_acc: 0.6548\n",
      "Epoch 64/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 0.6177 - acc: 0.7478 - val_loss: 0.8780 - val_acc: 0.6131\n",
      "Epoch 65/100\n",
      "1340/1340 [==============================] - 0s 73us/step - loss: 0.5866 - acc: 0.7552 - val_loss: 0.8833 - val_acc: 0.5893\n",
      "Epoch 66/100\n",
      "1340/1340 [==============================] - 0s 81us/step - loss: 0.5622 - acc: 0.7701 - val_loss: 0.8912 - val_acc: 0.5685\n",
      "Epoch 67/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 0.5543 - acc: 0.7739 - val_loss: 0.8898 - val_acc: 0.5685\n",
      "Epoch 68/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 0.5436 - acc: 0.7784 - val_loss: 0.8754 - val_acc: 0.5774\n",
      "Epoch 69/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.5224 - acc: 0.7888 - val_loss: 0.8617 - val_acc: 0.5923\n",
      "Epoch 70/100\n",
      "1340/1340 [==============================] - 0s 91us/step - loss: 0.5189 - acc: 0.7993 - val_loss: 0.8546 - val_acc: 0.6071\n",
      "Epoch 71/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 0.4801 - acc: 0.8097 - val_loss: 0.8522 - val_acc: 0.6190\n",
      "Epoch 72/100\n",
      "1340/1340 [==============================] - 0s 80us/step - loss: 0.4884 - acc: 0.8007 - val_loss: 0.8466 - val_acc: 0.6280\n",
      "Epoch 73/100\n",
      "1340/1340 [==============================] - 0s 79us/step - loss: 0.4997 - acc: 0.8187 - val_loss: 0.8374 - val_acc: 0.6488\n",
      "Epoch 74/100\n",
      "1340/1340 [==============================] - 0s 73us/step - loss: 0.4782 - acc: 0.8112 - val_loss: 0.8367 - val_acc: 0.6548\n",
      "Epoch 75/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 0.4452 - acc: 0.8261 - val_loss: 0.8353 - val_acc: 0.6488\n",
      "Epoch 76/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 0.4438 - acc: 0.8321 - val_loss: 0.8308 - val_acc: 0.6518\n",
      "Epoch 77/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.4249 - acc: 0.8343 - val_loss: 0.8268 - val_acc: 0.6458\n",
      "Epoch 78/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 0.4247 - acc: 0.8396 - val_loss: 0.8270 - val_acc: 0.6310\n",
      "Epoch 79/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 0.3951 - acc: 0.8396 - val_loss: 0.8316 - val_acc: 0.6220\n",
      "Epoch 80/100\n",
      "1340/1340 [==============================] - 0s 76us/step - loss: 0.4005 - acc: 0.8403 - val_loss: 0.8349 - val_acc: 0.6250\n",
      "Epoch 81/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 0.3926 - acc: 0.8507 - val_loss: 0.8390 - val_acc: 0.6250\n",
      "Epoch 82/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 0.3726 - acc: 0.8627 - val_loss: 0.8497 - val_acc: 0.6250\n",
      "Epoch 83/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 0.3791 - acc: 0.8530 - val_loss: 0.8558 - val_acc: 0.6220\n",
      "Epoch 84/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 0.3414 - acc: 0.8687 - val_loss: 0.8477 - val_acc: 0.6250\n",
      "Epoch 85/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 0.3582 - acc: 0.8582 - val_loss: 0.8331 - val_acc: 0.6429\n",
      "Epoch 86/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 0.3357 - acc: 0.8709 - val_loss: 0.8228 - val_acc: 0.6726\n",
      "Epoch 87/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.3382 - acc: 0.8679 - val_loss: 0.8240 - val_acc: 0.6815\n",
      "Epoch 88/100\n",
      "1340/1340 [==============================] - 0s 79us/step - loss: 0.3105 - acc: 0.8851 - val_loss: 0.8248 - val_acc: 0.6905\n",
      "Epoch 89/100\n",
      "1340/1340 [==============================] - 0s 78us/step - loss: 0.3249 - acc: 0.8821 - val_loss: 0.8221 - val_acc: 0.6905\n",
      "Epoch 90/100\n",
      "1340/1340 [==============================] - 0s 70us/step - loss: 0.3065 - acc: 0.8821 - val_loss: 0.8196 - val_acc: 0.6905\n",
      "Epoch 91/100\n",
      "1340/1340 [==============================] - 0s 75us/step - loss: 0.2620 - acc: 0.9067 - val_loss: 0.8245 - val_acc: 0.6696\n",
      "Epoch 92/100\n",
      "1340/1340 [==============================] - 0s 74us/step - loss: 0.3266 - acc: 0.8843 - val_loss: 0.8565 - val_acc: 0.6339\n",
      "Epoch 93/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 0.2705 - acc: 0.8993 - val_loss: 0.9164 - val_acc: 0.6012\n",
      "Epoch 94/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.2679 - acc: 0.8888 - val_loss: 0.9871 - val_acc: 0.5804\n",
      "Epoch 95/100\n",
      "1340/1340 [==============================] - 0s 73us/step - loss: 0.2857 - acc: 0.8910 - val_loss: 1.0267 - val_acc: 0.5714\n",
      "Epoch 96/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.2830 - acc: 0.8948 - val_loss: 1.0215 - val_acc: 0.5714\n",
      "Epoch 97/100\n",
      "1340/1340 [==============================] - 0s 78us/step - loss: 0.2563 - acc: 0.9067 - val_loss: 1.0010 - val_acc: 0.5774\n",
      "Epoch 98/100\n",
      "1340/1340 [==============================] - 0s 71us/step - loss: 0.2609 - acc: 0.9045 - val_loss: 0.9733 - val_acc: 0.5833\n",
      "Epoch 99/100\n",
      "1340/1340 [==============================] - 0s 77us/step - loss: 0.2504 - acc: 0.9104 - val_loss: 0.9478 - val_acc: 0.6042\n",
      "Epoch 100/100\n",
      "1340/1340 [==============================] - 0s 72us/step - loss: 0.2614 - acc: 0.9052 - val_loss: 0.9395 - val_acc: 0.6161\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "# make keras model\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(512, activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(len(df['Subclass'].value_counts()), activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# fit\n",
    "epochs = 100\n",
    "batch_size = 1000\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_for_keras,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test_for_keras),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        es,\n",
    "#         lr_decay\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save('../model/Keras_pos_fs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "df = pd.read_csv('../data/feature_selection_negative.csv')\n",
    "\n",
    "# divide objective and target\n",
    "objective = df.Subclass\n",
    "le = preprocessing.LabelEncoder()\n",
    "objective = le.fit_transform(objective)\n",
    "features = df.drop('Subclass', axis=1)\n",
    "\n",
    "# train test split\n",
    "random_state=np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    objective,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# transform  for keras's target label\n",
    "y_train_for_keras = np_utils.to_categorical(y_train)\n",
    "y_test_for_keras = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 373 samples, validate on 94 samples\n",
      "Epoch 1/100\n",
      "373/373 [==============================] - 2s 4ms/step - loss: 2.1786 - acc: 0.2225 - val_loss: 1.3061 - val_acc: 0.4149\n",
      "Epoch 2/100\n",
      "373/373 [==============================] - 0s 61us/step - loss: 1.9127 - acc: 0.2654 - val_loss: 1.3591 - val_acc: 0.4574\n",
      "Epoch 3/100\n",
      "373/373 [==============================] - 0s 89us/step - loss: 1.8433 - acc: 0.3137 - val_loss: 1.5017 - val_acc: 0.4894\n",
      "Epoch 4/100\n",
      "373/373 [==============================] - 0s 71us/step - loss: 1.6841 - acc: 0.3592 - val_loss: 1.6397 - val_acc: 0.5106\n",
      "Epoch 5/100\n",
      "373/373 [==============================] - 0s 74us/step - loss: 1.6561 - acc: 0.3753 - val_loss: 1.6861 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "373/373 [==============================] - 0s 71us/step - loss: 1.7015 - acc: 0.3700 - val_loss: 1.6637 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "373/373 [==============================] - 0s 53us/step - loss: 1.6823 - acc: 0.3753 - val_loss: 1.5997 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "373/373 [==============================] - 0s 66us/step - loss: 1.5479 - acc: 0.4048 - val_loss: 1.4979 - val_acc: 0.5106\n",
      "Epoch 9/100\n",
      "373/373 [==============================] - 0s 55us/step - loss: 1.4495 - acc: 0.4343 - val_loss: 1.3933 - val_acc: 0.5213\n",
      "Epoch 10/100\n",
      "373/373 [==============================] - 0s 61us/step - loss: 1.4363 - acc: 0.4021 - val_loss: 1.2840 - val_acc: 0.5213\n",
      "Epoch 11/100\n",
      "373/373 [==============================] - 0s 60us/step - loss: 1.5113 - acc: 0.4263 - val_loss: 1.1906 - val_acc: 0.5213\n",
      "Epoch 12/100\n",
      "373/373 [==============================] - 0s 70us/step - loss: 1.3097 - acc: 0.4209 - val_loss: 1.1214 - val_acc: 0.5426\n",
      "Epoch 13/100\n",
      "373/373 [==============================] - 0s 69us/step - loss: 1.3672 - acc: 0.4370 - val_loss: 1.0815 - val_acc: 0.5532\n",
      "Epoch 14/100\n",
      "373/373 [==============================] - 0s 62us/step - loss: 1.4232 - acc: 0.4155 - val_loss: 1.0451 - val_acc: 0.5638\n",
      "Epoch 15/100\n",
      "373/373 [==============================] - 0s 69us/step - loss: 1.3622 - acc: 0.4477 - val_loss: 1.0224 - val_acc: 0.5957\n",
      "Epoch 16/100\n",
      "373/373 [==============================] - 0s 72us/step - loss: 1.2242 - acc: 0.4584 - val_loss: 1.0095 - val_acc: 0.6064\n",
      "Epoch 17/100\n",
      "373/373 [==============================] - 0s 63us/step - loss: 1.2115 - acc: 0.4638 - val_loss: 0.9984 - val_acc: 0.6064\n",
      "Epoch 18/100\n",
      "373/373 [==============================] - 0s 55us/step - loss: 1.2546 - acc: 0.4745 - val_loss: 0.9879 - val_acc: 0.6170\n",
      "Epoch 19/100\n",
      "373/373 [==============================] - 0s 61us/step - loss: 1.2124 - acc: 0.5147 - val_loss: 0.9836 - val_acc: 0.6277\n",
      "Epoch 20/100\n",
      "373/373 [==============================] - 0s 57us/step - loss: 1.1640 - acc: 0.4960 - val_loss: 0.9908 - val_acc: 0.6277\n",
      "Epoch 21/100\n",
      "373/373 [==============================] - 0s 66us/step - loss: 1.1090 - acc: 0.5282 - val_loss: 1.0058 - val_acc: 0.6064\n",
      "Epoch 22/100\n",
      "373/373 [==============================] - 0s 73us/step - loss: 1.1057 - acc: 0.5255 - val_loss: 1.0259 - val_acc: 0.6064\n",
      "Epoch 23/100\n",
      "373/373 [==============================] - 0s 67us/step - loss: 1.1650 - acc: 0.5040 - val_loss: 1.0403 - val_acc: 0.6064\n",
      "Epoch 24/100\n",
      "373/373 [==============================] - 0s 53us/step - loss: 1.2057 - acc: 0.5040 - val_loss: 1.0581 - val_acc: 0.6064\n",
      "Epoch 25/100\n",
      "373/373 [==============================] - 0s 47us/step - loss: 1.0252 - acc: 0.5335 - val_loss: 1.0635 - val_acc: 0.5957\n",
      "Epoch 26/100\n",
      "373/373 [==============================] - 0s 48us/step - loss: 1.0078 - acc: 0.5576 - val_loss: 1.0737 - val_acc: 0.5851\n",
      "Epoch 27/100\n",
      "373/373 [==============================] - 0s 70us/step - loss: 1.0778 - acc: 0.5121 - val_loss: 1.0707 - val_acc: 0.5851\n",
      "Epoch 28/100\n",
      "373/373 [==============================] - 0s 66us/step - loss: 0.9954 - acc: 0.5737 - val_loss: 1.0625 - val_acc: 0.5957\n",
      "Epoch 29/100\n",
      "373/373 [==============================] - 0s 76us/step - loss: 1.0188 - acc: 0.5684 - val_loss: 1.0528 - val_acc: 0.5957\n",
      "Epoch 30/100\n",
      "373/373 [==============================] - 0s 67us/step - loss: 0.9492 - acc: 0.6059 - val_loss: 1.0421 - val_acc: 0.6064\n",
      "Epoch 31/100\n",
      "373/373 [==============================] - 0s 76us/step - loss: 0.9519 - acc: 0.5657 - val_loss: 1.0314 - val_acc: 0.6064\n",
      "Epoch 32/100\n",
      "373/373 [==============================] - 0s 66us/step - loss: 0.9597 - acc: 0.5818 - val_loss: 1.0203 - val_acc: 0.6064\n",
      "Epoch 33/100\n",
      "373/373 [==============================] - 0s 66us/step - loss: 0.8977 - acc: 0.6220 - val_loss: 1.0109 - val_acc: 0.6064\n",
      "Epoch 34/100\n",
      "373/373 [==============================] - 0s 70us/step - loss: 0.9405 - acc: 0.5952 - val_loss: 1.0027 - val_acc: 0.6064\n",
      "Epoch 35/100\n",
      "373/373 [==============================] - 0s 61us/step - loss: 0.9026 - acc: 0.5925 - val_loss: 0.9944 - val_acc: 0.6170\n",
      "Epoch 36/100\n",
      "373/373 [==============================] - 0s 73us/step - loss: 0.9043 - acc: 0.6059 - val_loss: 0.9878 - val_acc: 0.6170\n",
      "Epoch 37/100\n",
      "373/373 [==============================] - 0s 72us/step - loss: 0.9077 - acc: 0.5979 - val_loss: 0.9843 - val_acc: 0.6277\n",
      "Epoch 38/100\n",
      "373/373 [==============================] - 0s 64us/step - loss: 0.7955 - acc: 0.6622 - val_loss: 0.9841 - val_acc: 0.6170\n",
      "Epoch 39/100\n",
      "373/373 [==============================] - 0s 61us/step - loss: 0.9083 - acc: 0.6139 - val_loss: 0.9865 - val_acc: 0.6170\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "# make keras model\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(512, activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(len(df['Subclass'].value_counts()), activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# fit\n",
    "epochs = 100\n",
    "batch_size = 1000\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_for_keras,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test_for_keras),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        es,\n",
    "#         lr_decay\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save('../model/Keras_ng_fs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
